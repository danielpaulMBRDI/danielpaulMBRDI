{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpaulMBRDI/danielpaulMBRDI/blob/main/3_basics_cnn_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qscW3Ppun1-"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function"
      ],
      "metadata": {
        "id": "RcQl7T4H5r1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_convs(image, conv_layer, axis=False):\n",
        "    \"\"\"Plot convs with matplotlib. Sorry for this lazy code :D\"\"\"\n",
        "    filtered_image = conv_layer(image[None, :])\n",
        "    n = filtered_image.shape[1]\n",
        "    if n == 1:\n",
        "        fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n",
        "        # image = image.permute(1, 2, 0)\n",
        "        image = image / 2 + 0.5     # unnormalize\n",
        "        npimg = image.numpy()\n",
        "        ax1.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        ax1.set_title(\"Original\")\n",
        "        ax2.imshow(filtered_image.detach().squeeze())  \n",
        "        ax2.set_title(\"Filter 1\")\n",
        "        ax1.grid(False)\n",
        "        ax2.grid(False)\n",
        "        if not axis:\n",
        "            ax1.axis(False)\n",
        "            ax2.axis(False)\n",
        "        plt.tight_layout();\n",
        "    elif n == 2:\n",
        "        filtered_image_1 = filtered_image[:,0,:,:]\n",
        "        filtered_image_2 = filtered_image[:,1,:,:]\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(figsize=(10, 4), ncols=3)\n",
        "        image = image / 2 + 0.5     # unnormalize\n",
        "        npimg = image.numpy()\n",
        "        ax1.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        ax1.set_title(\"Original\")\n",
        "        ax2.imshow(filtered_image_1.detach().squeeze())  \n",
        "        ax2.set_title(\"Filter 1\")\n",
        "        ax3.imshow(filtered_image_2.detach().squeeze())  \n",
        "        ax3.set_title(\"Filter 2\")\n",
        "        ax1.grid(False)\n",
        "        ax2.grid(False)\n",
        "        ax3.grid(False)\n",
        "        if not axis:\n",
        "            ax1.axis(False)\n",
        "            ax2.axis(False)\n",
        "            ax3.axis(False)\n",
        "        plt.tight_layout();\n",
        "    elif n == 3:\n",
        "        filtered_image_1 = filtered_image[:,0,:,:]\n",
        "        filtered_image_2 = filtered_image[:,1,:,:]\n",
        "        filtered_image_3 = filtered_image[:,2,:,:]\n",
        "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(12, 4), ncols=4)\n",
        "        image = image / 2 + 0.5     # unnormalize\n",
        "        npimg = image.numpy()\n",
        "        ax1.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        ax1.set_title(\"Original\")\n",
        "        ax2.imshow(filtered_image_1.detach().squeeze())  \n",
        "        ax2.set_title(\"Filter 1\")\n",
        "        ax3.imshow(filtered_image_2.detach().squeeze())  \n",
        "        ax3.set_title(\"Filter 2\")\n",
        "        ax4.imshow(filtered_image_3.detach().squeeze())  \n",
        "        ax4.set_title(\"Filter 3\")\n",
        "        ax1.grid(False)\n",
        "        ax2.grid(False)\n",
        "        ax3.grid(False)\n",
        "        ax4.grid(False)\n",
        "        if not axis:\n",
        "            ax1.axis(False)\n",
        "            ax2.axis(False)\n",
        "            ax3.axis(False)\n",
        "            ax4.axis(False)\n",
        "        plt.tight_layout();"
      ],
      "metadata": {
        "id": "-qBkX8E0gxO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keXWLYv7un2C"
      },
      "source": [
        "## 0. Basics\n",
        "- Input of image-format data is usually 4-D array\n",
        "<br> **(num_instance, width, height, depth)** </br>\n",
        "    - **num_instance:** number of data instances. Usually designated as **None** to accomodate fluctuating data size\n",
        "    - **width:** width of an image\n",
        "    - **height:** height of an image\n",
        "    - **depth:** depth of an image. Color images are usually with depth = 3 (3 channels for RGB). Black/white images are usually with depth = 1 (only one channel)\n",
        "    \n",
        "<img src=\"http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png\" style=\"width: 400px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4HfWfnVun2D"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True)\n",
        "print(len(trainset))\n",
        "print(trainset.__getitem__(0)[0].size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nFDDKJMun2G"
      },
      "source": [
        "# showing figures\n",
        "fig = plt.figure(figsize = (10, 10))\n",
        "for i in range(9):\n",
        "  fig.add_subplot(3, 3, i+1)\n",
        "  plt.imshow(trainset.__getitem__(i)[0])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QEPYGasun2J"
      },
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True)\n",
        "print(len(trainset))\n",
        "print(trainset.__getitem__(0)[0].size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCsTEA6Eun2M"
      },
      "source": [
        "# showing figures\n",
        "fig = plt.figure(figsize = (10, 10))\n",
        "for i in range(9):\n",
        "  fig.add_subplot(3, 3, i+1)\n",
        "  plt.imshow(trainset.__getitem__(i)[0])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. FIlter/kernels\n",
        "Number of filters can be designated\n",
        "\n",
        "Number of filters equals to the depth of next layer"
      ],
      "metadata": {
        "id": "k7sbSTrpftgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, convolutional layers are defined as torch.nn.Conv2d, there are 5 important arguments we need to know:\n",
        "\n",
        "1. in_channels: how many features are we passing in. Our features are our colour bands, in greyscale, we have 1 feature, in colour, we have 3 channels.\n",
        "2. out_channels: how many kernels do we want to use. Analogous to the number of hidden nodes in a hidden layer of a fully connected network.\n",
        "3. kernel_size: the size of the kernel. Above we were using 3x3. Common sizes are 3x3, 5x5, 7x7.\n",
        "4. stride: the \"step-size\" of the kernel.\n",
        "5. padding: the number of pixels we should pad to the outside of the image so we can get edge pixels."
      ],
      "metadata": {
        "id": "5h9gXXIRg8py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True,  transform=transform)"
      ],
      "metadata": {
        "id": "cyeToUo7h1Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (3,3)\n",
        "image = torch.tensor(np.array(trainset.__getitem__(0)[0]))\n",
        "# torch.nn.Conv2d(3, number of kernels, kernel_size=(5, 5))\n",
        "conv_layer = torch.nn.Conv2d(3, 1, kernel_size=(5, 5))\n",
        "plot_convs(image, conv_layer)\n",
        "print(image.shape)"
      ],
      "metadata": {
        "id": "QMfcM3bufrgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 kernels of (3,3)\n",
        "# Define for 2 kernels\n",
        "conv_layer = torch.nn.Conv2d()\n",
        "plot_convs(image, conv_layer)"
      ],
      "metadata": {
        "id": "7AmfmyA0hJU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 kernels of (5,5)\n",
        "# Define for 3 kernels\n",
        "conv_layer = torch.nn.Conv2d()\n",
        "plot_convs(image, conv_layer)"
      ],
      "metadata": {
        "id": "eRcLxOeGkuJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Strides"
      ],
      "metadata": {
        "id": "FAXdgl8bmCz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (5,5) with stride of 2\n",
        "conv_layer = torch.nn.Conv2d(3, 1, kernel_size=(5, 5), stride=2)\n",
        "plot_convs(image, conv_layer, axis=True)"
      ],
      "metadata": {
        "id": "HvdXExiwlbed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (5,5) with stride of 3\n",
        "conv_layer = torch.nn.Conv2d()\n",
        "plot_convs(image, conv_layer, axis=True)"
      ],
      "metadata": {
        "id": "Y7j34z0GGva2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (5,5) with stride of 4\n",
        "conv_layer = torch.nn.Conv2d()\n",
        "plot_convs(image, conv_layer, axis=True)"
      ],
      "metadata": {
        "id": "9npgjCu-GzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's important with CNNs is that the size of our input data does not impact how many parameters we have in our convolutonal layers. For example, your kernels don't care how big your image is (i.e., 28 x 28 or 256 x 256), all that matters is:\n",
        "\n",
        "* How many features (\"channels\") you have: in_channels\n",
        "* How many filters you use in each layer: out_channels\n",
        "* How big the filters are: kernel_size"
      ],
      "metadata": {
        "id": "sGE5kDKbmVQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Padding\n",
        "Zero padding can be applied in convolution/pooling as we have seen above. But custom padding can be applied as well\n",
        "* nn.ConstandPad1d(padding, value): apply constant padding on 1D data\n",
        "  * padding: the shape of padding (if tuple, (padingLeft, padingRight))\n",
        "  * value: the value of padding\n",
        "* nn.ConstantPad2d(padding, value): apply constant padding on 2D data\n",
        "  * padding: the shape of padding (if tuple, (padingLeft, padingRight, paddingTop, padingBottom))\n",
        "  * value: the value of padding\n",
        "* nn.ZeroPad2d(padding): apply zero padding on 2D data\n",
        "  * padding: the shape of padding (if tuple, (padingLeft, padingRight, paddingTop, padingBottom))"
      ],
      "metadata": {
        "id": "Mp5ae7XdeHC1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7JLRN5E6BYU"
      },
      "source": [
        "# p = nn.ConstantPad1d((l, r, t, b), val) \n",
        "p = nn.ConstantPad1d((1, 1, 1, 1), -1) # 1d padding with constant 0.75\n",
        "x = torch.ones(1, 1, 3)\n",
        "print(p(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = nn.ZeroPad2d((1,0,0,0))  # apply zero padding only on the left of first column\n",
        "x = torch.ones(1, 1, 3, 3)\n",
        "print(p(x))"
      ],
      "metadata": {
        "id": "6tkFmJKtfowJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use a kernel with no padding, our output image will be smaller as we noted earlier."
      ],
      "metadata": {
        "id": "-gFNUwK8lOAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (51,51)\n",
        "conv_layer = torch.nn.Conv2d(3, 1, kernel_size=(5, 5))\n",
        "plot_convs(image, conv_layer, axis=True)"
      ],
      "metadata": {
        "id": "VSvcZnIzlJA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw, we can add padding to the outside of the image to avoid this:"
      ],
      "metadata": {
        "id": "9DE_BRVhlzWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 kernel of (51,51) with padding\n",
        "conv_layer = torch.nn.Conv2d(3, 1, kernel_size=(5, 5), padding=2)\n",
        "plot_convs(image, conv_layer, axis=True)"
      ],
      "metadata": {
        "id": "WyoSQ3bFlQnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting $padding = kernel\\_size // 2$ will always result in an output the same shape as the input. Think about why this is..."
      ],
      "metadata": {
        "id": "IMQygbCbl6Lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pooling\n",
        "\n",
        "Pooling is how we can reduce the number of parameters we get out of a torch.nn.Flatten(). It's pretty simple, we just aggregate the data, usually using the maximum or average of a window of pixels. \n",
        "\n",
        "We use \"pooling layers\" to reduce the shape of our image as it's passing through the network. So when we eventually torch.nn.Flatten(), we'll have less features in that flattened layer! We can implement pooling with torch.nn.MaxPool2d()"
      ],
      "metadata": {
        "id": "DyjGtA7rnJQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d((2, 2)),\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d((2, 2)),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(1250, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.main(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "T8rgSOLynE5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "summary(model, (1, 100, 100))"
      ],
      "metadata": {
        "id": "jVRzamrUnV0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Flattening\n",
        "\n",
        "torch.nn.Flatten()\n",
        "\n",
        "To be connected to fully connected layer (dense layer), convolutional/pooling layer should be \"flattened\"\n",
        "\n",
        "Resulting shape = (Number of instances, width X height X depth)"
      ],
      "metadata": {
        "id": "8-TFvBJmmgsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(20000, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.main(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_lLeRBt0mK7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "summary(model, (1, 100, 100))"
      ],
      "metadata": {
        "id": "65N0gp7LnDq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoqqskhdnXsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}