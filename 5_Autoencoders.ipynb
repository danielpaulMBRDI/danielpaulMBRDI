{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpaulMBRDI/danielpaulMBRDI/blob/main/5_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoEncoders\n",
        "\n",
        "Autoencoders automatically consists of two structures: the encoder and the decoder. The encoder network downsamples the data into lower dimensions and the decoder network reconstructs the original data from the lower dimension representation. The lower dimension representation is usually called latent space representation. "
      ],
      "metadata": {
        "id": "uRn0w7N13pb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*8ixTe1VHLsmKB3AquWdxpQ.png)\n",
        "\n",
        "[Source](https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f)"
      ],
      "metadata": {
        "id": "5m0zm4cN3st1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "X7DXRBgyG1Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries"
      ],
      "metadata": {
        "id": "DsQ1_LxP4P45"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMR-3U0c3ki-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import plotnine\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "what_were_covering = {1: \"data (prepare and load)\",\n",
        "    2: \"build model\",\n",
        "    3: \"fitting the model to data (training)\",\n",
        "    4: \"making predictions and evaluating a model (inference)\",\n",
        "    5: \"saving and loading a model\",\n",
        "    6: \"putting it all together\"\n",
        "}"
      ],
      "metadata": {
        "id": "3ojppkAu_rjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation"
      ],
      "metadata": {
        "id": "ecQpN57h4RWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "metadata": {
        "id": "AcKRjW2oAe-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), 'train samples')\n",
        "print(len(test_data), 'test samples')"
      ],
      "metadata": {
        "id": "p5loT3cz4ZNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Train and Test Loader"
      ],
      "metadata": {
        "id": "tzQszgm-_8Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dataloaders\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "yXW6JYXVn-aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualize Data"
      ],
      "metadata": {
        "id": "eNC3X0ezoBjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "7gbiKN2toD23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Building"
      ],
      "metadata": {
        "id": "PNfOgVp0AImu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Autoencoder\n",
        "\n",
        "We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building a simple autoencoder. The encoder and decoder should be made of one linear layer. The units that connect the encoder and decoder will be the compressed representation.\n",
        "\n",
        "Since the images are normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range."
      ],
      "metadata": {
        "id": "cPx9nlcM4wzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the NN architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        ## encoder ##\n",
        "        self.encoder = nn.Linear(784, encoding_dim)\n",
        "        ## decoder ##\n",
        "        self.decoder = nn.Linear(encoding_dim, 784)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define feedforward behavior \n",
        "        # and scale the *output* layer with a sigmoid activation function\n",
        "        \n",
        "        # pass x into encoder\n",
        "        out = F.relu(self.encoder(x))\n",
        "        # pass out into decoder\n",
        "        out = torch.sigmoid(self.decoder(out))\n",
        "        \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "VMEvNl-S75Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Check if GPU is available"
      ],
      "metadata": {
        "id": "uY1eKd_qANQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "rbOuQ9-DC3S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "iIXem1bxDi9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Initialize Model"
      ],
      "metadata": {
        "id": "bDidW3KEAR-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the NN\n",
        "encoding_dim = 64\n",
        "model = Autoencoder(encoding_dim)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "aW5dN-XR45Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model"
      ],
      "metadata": {
        "id": "tM_7CcDrAWxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define a Loss function and optimizer"
      ],
      "metadata": {
        "id": "0ku16baFAYbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# specify loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "6pWwhBaooknQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Train the network"
      ],
      "metadata": {
        "id": "6U9_VCiyAbPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data in train_loader:\n",
        "        # _ stands in for labels, here\n",
        "        images, _ = data\n",
        "        # flatten images\n",
        "        images = images.view(images.size(0), -1)\n",
        "        images = images.to(device)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images)\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, images)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "            \n",
        "    # print avg training statistics \n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch, \n",
        "        train_loss\n",
        "        ))"
      ],
      "metadata": {
        "id": "haBuCspa47w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "omGuQ5tTAhkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize the Encoded Inputs"
      ],
      "metadata": {
        "id": "wFpXVZ1JAm3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images_flatten = images.view(images.size(0), -1)\n",
        "# get sample outputs\n",
        "images_flatten = images_flatten.to(device)\n",
        "output = model.encoder(images_flatten)\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output = output.view(batch_size, 1, 8, 8)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "for img, ax in zip(images, axes):\n",
        "    ax.imshow(np.squeeze(img), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "for img, ax in zip(output, axes):\n",
        "    ax.imshow(np.squeeze(img), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "FqhtiZ3sB0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Visualize Reconstructed Output"
      ],
      "metadata": {
        "id": "Vvtr5EN8Arpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images_flatten = images.view(images.size(0), -1)\n",
        "# get sample outputs\n",
        "images_flatten = images_flatten.to(device)\n",
        "output = model(images_flatten)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output = output.view(batch_size, 1, 28, 28)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# input images on top row, reconstructions on bottom\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "q_DhYNZb4-ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Visualize Distribution of Encodings in 2D Space"
      ],
      "metadata": {
        "id": "a79qHyb9Av47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.1 Create a t-SNE Plot"
      ],
      "metadata": {
        "id": "ezwEf82BA2yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "test_encoded_df = pd.DataFrame(columns = ['Encoding_1', 'Encoding_2', 'Target'])\n",
        "outputs = []\n",
        "labels = []\n",
        "\n",
        "for data in test_loader:\n",
        "        # _ stands in for labels, here\n",
        "        images, label = data\n",
        "        images = images.view(images.size(0), -1)\n",
        "        images = images.to(device)\n",
        "        output = model.encoder(images)\n",
        "        for i in range(0, batch_size):\n",
        "            # output is resized into a batch of images\n",
        "            output_n = output[i]\n",
        "            lab = label[i].item()\n",
        "            \n",
        "            outputs.append(output_n.detach().cpu().detach().numpy())\n",
        "            labels.append(lab)\n",
        "\n",
        "tsne_op = tsne.fit_transform(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "sOxHbxFmrSzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(labels)):\n",
        "  test_encoded_df = test_encoded_df.append({'Encoding_1' :tsne_op[i][0] , 'Encoding_2' :tsne_op[i][1], 'Target' : labels[i]},\n",
        "ignore_index = True)"
      ],
      "metadata": {
        "id": "vZCX-gr70gJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_mnist_linear = (\n",
        "    plotnine.ggplot(data=test_encoded_df)\n",
        "    + plotnine.geom_point(\n",
        "        mapping=plotnine.aes(x=\"Encoding_1\", y=\"Encoding_2\", fill=\"factor(Target)\"),\n",
        "        size=2,\n",
        "        color=\"black\",\n",
        "    )\n",
        "    + plotnine.xlab(xlab=\"Encoding dimension 1\")\n",
        "    + plotnine.ylab(ylab=\"Encoding dimension 2\")\n",
        "    + plotnine.ggtitle(title=\"MNIST Linear autoencoder with 2-dimensional encoding\")\n",
        "    + plotnine.theme_matplotlib()\n",
        ")\n"
      ],
      "metadata": {
        "id": "L1D4dw7X5BiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 Visualize t-SNE Plot"
      ],
      "metadata": {
        "id": "ln_vGoaVBAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_mnist_linear"
      ],
      "metadata": {
        "id": "vFUcPP2X6eXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Autoencoder with CIFAR 10"
      ],
      "metadata": {
        "id": "oqL10iSMrsm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download Weights of Pretrained model"
      ],
      "metadata": {
        "id": "jgQd_d1bHHVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1zEv6kxVzwUvPp0amC3LLlDlsjupvmE1A"
      ],
      "metadata": {
        "id": "PYTVj_h5HDdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation"
      ],
      "metadata": {
        "id": "jVWgWrv4HQwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "metadata": {
        "id": "6suEJhnarsIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Train Test split"
      ],
      "metadata": {
        "id": "HFLsjqBJHUU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dataloaders\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "FiG3UGcTryq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Visualize Images"
      ],
      "metadata": {
        "id": "rPZGdV26H90Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    img = np.transpose(npimg, (1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Ist7tG5MuNuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "metadata": {
        "id": "2s-un05puGIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Building"
      ],
      "metadata": {
        "id": "sbUxkcycHbnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the NN architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        ## encoder ##\n",
        "        self.encoder = nn.Linear(3072, encoding_dim)\n",
        "        ## decoder ##\n",
        "        self.decoder = nn.Linear(encoding_dim, 3072)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define feedforward behavior \n",
        "        # and scale the *output* layer with a sigmoid activation function\n",
        "        \n",
        "        # pass x into encoder\n",
        "        out = F.relu(self.encoder(x))\n",
        "        # pass out into decoder\n",
        "        out = torch.sigmoid(self.decoder(out))\n",
        "        \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "9icmBnjlsNTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Check if GPU available"
      ],
      "metadata": {
        "id": "cAdL0lYLHeOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "asYxF-T3Hkrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "11Tex0gmHkrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Initialize Model"
      ],
      "metadata": {
        "id": "-K3sBKDaHmR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the NN \n",
        "# Define Bottleneck\n",
        "encoding_dim = 0\n",
        "model = Autoencoder(encoding_dim)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qGNxGxs5r5xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Load weights"
      ],
      "metadata": {
        "id": "KWuUZKrHHqIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a path\n",
        "PATH = \"cifar_linear.pt\"\n",
        "\n",
        "# Load\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "tHRew3fRHssT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "Oaxxaq2OIB_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize Encodings"
      ],
      "metadata": {
        "id": "rFYOZr-AIGeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images_flatten = images.view(images.size(0), -1)\n",
        "# get sample outputs\n",
        "images_flatten = images_flatten.to(device)\n",
        "output = model.encoder(images_flatten)\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output = output.view(batch_size, 1, 16, 16)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "\n",
        "for img, ax in zip(images, axes):\n",
        "    npimg = img.numpy()\n",
        "    # print(npimg.shape)\n",
        "    img = np.transpose(npimg, (1, 2, 0))\n",
        "    ax.imshow(img)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "# for img, ax in zip(output, axes):\n",
        "#     ax.imshow(np.squeeze(img))\n",
        "#     ax.get_xaxis().set_visible(False)\n",
        "#     ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "8ZjBAtYysWE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Visualize Reconstructed Images"
      ],
      "metadata": {
        "id": "HW9LTSE2IMdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images_flatten = images.view(images.size(0), -1)\n",
        "# get sample outputs\n",
        "images_flatten = images_flatten.to(device)\n",
        "output = model(images_flatten)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output = output.view(batch_size, 3, 32, 32)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# input images on top row, reconstructions on bottom\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        # print(npimg.shape)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        ax.imshow(img)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "NtTriwWZvzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Visualize t-SNE plots for the embeddings"
      ],
      "metadata": {
        "id": "-jQE0XFIIRrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "test_encoded_df = pd.DataFrame(columns = ['Encoding_1', 'Encoding_2', 'Target'])\n",
        "outputs = []\n",
        "labels = []\n",
        "\n",
        "for data in test_loader:\n",
        "        # _ stands in for labels, here\n",
        "        images, label = data\n",
        "        images = images.view(images.size(0), -1)\n",
        "        images = images.to(device)\n",
        "        output = model.encoder(images)\n",
        "        for i in range(0, batch_size):\n",
        "            # output is resized into a batch of images\n",
        "            output_n = output[i]\n",
        "            lab = label[i].item()\n",
        "            \n",
        "            outputs.append(output_n.detach().cpu().detach().numpy())\n",
        "            labels.append(lab)\n",
        "\n",
        "tsne_op = tsne.fit_transform(outputs)"
      ],
      "metadata": {
        "id": "T1cCXGrJw-bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(labels)):\n",
        "  test_encoded_df = test_encoded_df.append({'Encoding_1' :tsne_op[i][0] , 'Encoding_2' :tsne_op[i][1], 'Target' : labels[i]},\n",
        "ignore_index = True)"
      ],
      "metadata": {
        "id": "fXxQap5RxGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_cifar_linear = (\n",
        "    plotnine.ggplot(data=test_encoded_df)\n",
        "    + plotnine.geom_point(\n",
        "        mapping=plotnine.aes(x=\"Encoding_1\", y=\"Encoding_2\", fill=\"factor(Target)\"),\n",
        "        size=2,\n",
        "        color=\"black\",\n",
        "    )\n",
        "    + plotnine.xlab(xlab=\"Encoding dimension 1\")\n",
        "    + plotnine.ylab(ylab=\"Encoding dimension 2\")\n",
        "    + plotnine.ggtitle(title=\"CIFAR Linear autoencoder with 2-dimensional encoding\")\n",
        "    + plotnine.theme_matplotlib()\n",
        ")\n"
      ],
      "metadata": {
        "id": "-UAAgkJwxGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_cifar_linear"
      ],
      "metadata": {
        "id": "3EdjHWNkxGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional autoencoder\n",
        "\n",
        "We may also ask ourselves: can autoencoders be used with Convolutions instead of Fully-connected layers ?\n",
        "\n",
        "The answer is yes and the principle is the same, but using images (3D vectors) instead of flattened 1D vectors. The input image is downsampled to give a latent representation of smaller dimensions and force the autoencoder to learn a compressed version of the images."
      ],
      "metadata": {
        "id": "QEvnR0UE63CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation"
      ],
      "metadata": {
        "id": "osuQZBO5ItTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "metadata": {
        "id": "vfZWQU0kxd1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Train Test Loaders"
      ],
      "metadata": {
        "id": "h3k1XPzVIwA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dataloaders\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "ukgEfLg5xd1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Building"
      ],
      "metadata": {
        "id": "0YukcYN6I0XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Autoencoder,self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            # conv layer (depth from 1 --> 16), 3x3 kernels\n",
        "            nn.Conv2d(1, 16, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "            nn.Conv2d(16, 4, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.decoder = nn.Sequential(  \n",
        "            ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2           \n",
        "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 1, 2, stride=2),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fz5ufbjf1owS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Check if GPU available"
      ],
      "metadata": {
        "id": "dCwoSrfQI6gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "daxUbCywI6gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "SwTLiMpAI6gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Initialize Model"
      ],
      "metadata": {
        "id": "0V6EO2cGI6gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder()\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "y2ZPg0DP1riz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training a Model"
      ],
      "metadata": {
        "id": "Av2Toof-JPfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define a Loss function and optimizer"
      ],
      "metadata": {
        "id": "7Q5APREHJTXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# specify loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "2ql8vdcG1wno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Training a Network"
      ],
      "metadata": {
        "id": "3WBENj5DJWN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "for epoch in range(n_epochs+1):\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data in train_loader:\n",
        "        # _ stands in for labels, here\n",
        "        # no need to flatten images\n",
        "        images, _ = data\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images.to(device))\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, images.to(device))\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "            \n",
        "    # print avg training statistics \n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch, \n",
        "        train_loss\n",
        "        ))\n",
        "    "
      ],
      "metadata": {
        "id": "q8ij4YlM11zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "g1ahCmsAJ05P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize the Reconstructed Inputs"
      ],
      "metadata": {
        "id": "NrCtUFhpJ05P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images.to(device))\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "# output is resized into a batch of iages\n",
        "output = output.view(batch_size, 1, 28, 28)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# input images on top row, reconstructions on bottom\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "DiAb57x92No3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Visualize the embeddings using t-SNE plots"
      ],
      "metadata": {
        "id": "6cFduDYvJ-dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "test_encoded_df = pd.DataFrame(columns = ['Encoding_1', 'Encoding_2', 'Target'])\n",
        "outputs = []\n",
        "labels = []\n",
        "\n",
        "for data in test_loader:\n",
        "        # _ stands in for labels, here\n",
        "        images, label = data\n",
        "        # get sample outputs\n",
        "        output = model(images.to(device))\n",
        "        # Make 1D\n",
        "        output = output.view(images.size(0), -1)\n",
        "        for i in range(0, batch_size):\n",
        "            # output is resized into a batch of images\n",
        "            output_n = output[i]\n",
        "            lab = label[i].item()\n",
        "            \n",
        "            outputs.append(output_n.cpu().detach().numpy())\n",
        "            labels.append(lab)\n",
        "\n",
        "tsne_op = tsne.fit_transform(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "jTfPlOot5zkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(labels)):\n",
        "  test_encoded_df = test_encoded_df.append({'Encoding_1' :tsne_op[i][0] , 'Encoding_2' :tsne_op[i][1], 'Target' : labels[i]},\n",
        "ignore_index = True)"
      ],
      "metadata": {
        "id": "__5--SL153h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_mnist_conv = (\n",
        "    plotnine.ggplot(data=test_encoded_df)\n",
        "    + plotnine.geom_point(\n",
        "        mapping=plotnine.aes(x=\"Encoding_1\", y=\"Encoding_2\", fill=\"factor(Target)\"),\n",
        "        size=2,\n",
        "        color=\"black\",\n",
        "    )\n",
        "    + plotnine.xlab(xlab=\"Encoding dimension 1\")\n",
        "    + plotnine.ylab(ylab=\"Encoding dimension 2\")\n",
        "    + plotnine.ggtitle(title=\"MNIST Conv autoencoder with 2-dimensional encoding\")\n",
        "    + plotnine.theme_matplotlib()\n",
        ")\n"
      ],
      "metadata": {
        "id": "kvacLJra56id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_mnist_conv"
      ],
      "metadata": {
        "id": "fUNub6n259bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 Compare with plot of Linear Autoencoder"
      ],
      "metadata": {
        "id": "PsGGN_XlKQQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_mnist_linear"
      ],
      "metadata": {
        "id": "OYSOeomIKQQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR 10 Conv Autoencoder"
      ],
      "metadata": {
        "id": "-M2UnQdT06oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download Weights of Pretrained model"
      ],
      "metadata": {
        "id": "jYdo8gmrMT78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-WIgExr9O19tFgv_37a5tvGzwp4AixGa"
      ],
      "metadata": {
        "id": "OHwc9AxIMT8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation"
      ],
      "metadata": {
        "id": "mDpm1gd4MT8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "metadata": {
        "id": "3k6eZm5bMT8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Train Test split"
      ],
      "metadata": {
        "id": "496-wapNMT8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dataloaders\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "mepHX2YoMT8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Building"
      ],
      "metadata": {
        "id": "6q9GyRuXMtAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Autoencoder,self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            # conv layer (depth from 1 --> 16), 3x3 kernels\n",
        "            nn.Conv2d(3, 16, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "            nn.Conv2d(16, 4, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.decoder = nn.Sequential(  \n",
        "            ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2           \n",
        "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RUKuOneFMtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Check if GPU available"
      ],
      "metadata": {
        "id": "xyeH42aLMtAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "re_43JjSMtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "dJhAtLCHMtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Initialize Model"
      ],
      "metadata": {
        "id": "UomVIuuVMtAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the NN\n",
        "model = Autoencoder()\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qJ9_HmTlMtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Load weights"
      ],
      "metadata": {
        "id": "pfY5GtHQMtAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a path\n",
        "PATH = \"cifar_conv.pt\"\n",
        "\n",
        "# Load\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "rIL__m2sMtAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "NCUgVZH5NHv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize Reconstructed Images"
      ],
      "metadata": {
        "id": "yhosgHq5NKMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "output = model(images.to(device))\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "# output is resized into a batch of images\n",
        "\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach().cpu().detach().numpy()\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# input images on top row, reconstructions on bottom\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        # print(npimg.shape)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        ax.imshow(img)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "NJ8wzBVK03aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Visualize embedding space using t-SNE plots"
      ],
      "metadata": {
        "id": "Iwp5psyhNMuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "test_encoded_df = pd.DataFrame(columns = ['Encoding_1', 'Encoding_2', 'Target'])\n",
        "outputs = []\n",
        "labels = []\n",
        "\n",
        "for data in test_loader:\n",
        "        # _ stands in for labels, here\n",
        "        images, label = data\n",
        "        output = model(images.to(device))\n",
        "\n",
        "        for i in range(0, batch_size):\n",
        "            # output is resized into a batch of images\n",
        "            output_n = output[i]\n",
        "            lab = label[i].item()\n",
        "            \n",
        "            outputs.append(output_n.detach().cpu().detach().numpy().flatten())\n",
        "            labels.append(lab)\n",
        "\n",
        "tsne_op = tsne.fit_transform(outputs)"
      ],
      "metadata": {
        "id": "n6PRV68003aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(labels)):\n",
        "  test_encoded_df = test_encoded_df.append({'Encoding_1' :tsne_op[i][0] , 'Encoding_2' :tsne_op[i][1], 'Target' : labels[i]},\n",
        "ignore_index = True)"
      ],
      "metadata": {
        "id": "gb_knwfl03aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_cifar_conv = (\n",
        "    plotnine.ggplot(data=test_encoded_df)\n",
        "    + plotnine.geom_point(\n",
        "        mapping=plotnine.aes(x=\"Encoding_1\", y=\"Encoding_2\", fill=\"factor(Target)\"),\n",
        "        size=2,\n",
        "        color=\"black\",\n",
        "    )\n",
        "    + plotnine.xlab(xlab=\"Encoding dimension 1\")\n",
        "    + plotnine.ylab(ylab=\"Encoding dimension 2\")\n",
        "    + plotnine.ggtitle(title=\"CIFAR Conv autoencoder with 2-dimensional encoding\")\n",
        "    + plotnine.theme_matplotlib()\n",
        ")\n"
      ],
      "metadata": {
        "id": "txF_kQoa03aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_cifar_conv"
      ],
      "metadata": {
        "id": "iaXonNjd3xdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 Compare with t-SNE plot of Linear Auto Encoder"
      ],
      "metadata": {
        "id": "-DkJnJntNTrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_plot_cifar_linear"
      ],
      "metadata": {
        "id": "EfmpI9ZuNkBF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}